<div align="center">

# ðŸ”¥ HP-JAX  
### **High-Performance JAX Compute Framework for Scientific Computing, Distributed Systems, and TPU/Pi Clusters**

**Matrix Algebra â€¢ Auto-Diff â€¢ Distributed HPC â€¢ Raspberry Pi Cluster â€¢ TPU Acceleration**

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)]()
[![JAX](https://img.shields.io/badge/JAX-0.4%2B-green.svg)]()
[![License](https://img.shields.io/badge/License-Apache%202.0-yellow.svg)]()
[![Status](https://img.shields.io/badge/Status-Alpha-red.svg)]()

</div>

---

# âœ¨ Overview

**HP-JAX** is a **high-performance scientific computing framework** built on top of **Google JAX**,  
designed for **matrix computations**, **automatic differentiation**, and **distributed linear algebra**  
on **CPU**, **Raspberry Pi clusters**, and **TPU**.

It aims to bridge the gap between:

- âš¡ *High-performance numerical linear algebra*  
- ðŸ”¬ *Scientific computing & optimization*  
- ðŸ§  *JAX auto-diff (grad/Jacobian/Hessian)*  
- ðŸ”— *Distributed HPC (MPI)*  
- ðŸ§© *Low-cost clusters (Raspberry Pi & ARM64)*  
- ðŸš€ *Future: Cloud TPU acceleration*  

> **HP-JAX = SciPy + JAX + Distributed HPC, but lightweight and fully differentiable.**

GPU support will be added later â€” current focus is **CPU, ARM, MPI, and TPU**.

---

# ðŸ”¥ Features

### ðŸ§® **Matrix Algebra (CPU / Pi / TPU)**
- `matmul`, `inverse`, `det`, `transpose`
- Eigen decomposition (`eig`)
- SVD (`svd`)
- Cholesky (`cholesky`)
- QR decomposition (coming in v0.2)
- LU decomposition (coming in v0.2)
- Hessenberg / Schur (coming in v0.2)

### ðŸ§  **Automatic Differentiation**
- `gradient(f)`
- `jacobian(f)`
- `hessian(f)`
- Directional derivatives

Everything is **JIT-accelerated** and fully differentiable via JAX.

### ðŸ§© **Matrix Partitioning**
- 1-D row split (v0.1)
- 2-D block partition (v0.2)
- Ready for distributed workloads

### ðŸŒ **Distributed Linear Algebra (MPI)**
> **Optional â€” only installed if needed.**  
